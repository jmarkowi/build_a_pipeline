{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "950ca852",
   "metadata": {},
   "source": [
    "# <center>So, You Want To Build A Pipeline?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e1a00",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 12;font-style: italic;\"><img style=\"display: block;\" src=\"https://live.staticflickr.com/3724/11720986694_52084f53d0_b.jpg\" alt=\"Trans-canyon Pipeline (Historic) 2400\">*slaps pipe* This baby can hold so much data! | <a href=\"https://www.flickr.com/photos/50693818@N08/11720986694\">\"Trans-canyon Pipeline (Historic) 2400\"</a><span> by <a href=\"https://www.flickr.com/photos/50693818@N08\">Grand Canyon NPS</a></span> is licensed under <a href=\"https://creativecommons.org/licenses/by/2.0/?ref=ccsearch&atype=html\" style=\"margin-right: 5px;\">CC BY 2.0</a><a href=\"https://creativecommons.org/licenses/by/2.0/?ref=ccsearch&atype=html\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"display: inline-block;white-space: none;margin-top: 2px;margin-left: 3px;height: 12px !important;\"><img style=\"height: inherit;margin-right: 3px;display: inline-block;\" src=\"https://search.creativecommons.org/static/img/cc_icon.svg?image_id=66be56ca-6066-4bb4-87b8-7eb9b1adfdaf\" /><img style=\"height: inherit;margin-right: 3px;display: inline-block;\" src=\"https://search.creativecommons.org/static/img/cc-by_icon.svg\" /></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f57976",
   "metadata": {},
   "source": [
    "**Look, I'm only going to say this once:** One of the cardinal rules of writing code is Don't Repeat Yourself (DRY). It is a great universal rule that applies to all languages and it's especially important when it comes to handling data. DRY is why we write for loops instead of copying and pasting; it's why we declare functions instead of copying and pasting; and it's why we use pipelines to pre-process data, fit models, and cross-validate them too.\n",
    "\n",
    "Repetition in code isn't just cluttering and confusing--it can lead to serious errors as you write and re-write over variable values. Remember that time that you forgot you had already pre-processed `X_train` and you ended up filling the entire DataFrame will `np.nan`, then spent an hour trying to figure out why your model had the accuracy of a potato? Start using pipelines and let that experience become a tale you tell your children to bore them to sleep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb98914",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "![penguins](./images/palmerpenguins.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b60e0f",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "## Penguins in a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d3cc6",
   "metadata": {},
   "source": [
    "In this little how-to we'll see some pipelines in action and learn a few strategies along the way to make pipelines even more convenient. We'll use [Allison Horst's penguins dataset](https://github.com/allisonhorst/palmerpenguins), imported via Seaborn, to build a model that can predict the species of a penguin. The features listed for each penguin are home island, bill and flipper measurements, body mass, and sex. The three species of penguin in the dataset: Adelie, Gentoo, and Chinstrap. All of them are adorable, but especially the Adelie.\n",
    "\n",
    "*Note: I usually see one giant code block at the top of a notebook for importing dependencies, but I think it's more helpful to do it as we go so it's easier to see where everything comes from.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec83cb4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    Male  \n",
       "1       3800.0  Female  \n",
       "2       3250.0  Female  \n",
       "3          NaN     NaN  \n",
       "4       3450.0  Female  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(344, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To print some nice tables (https://pypi.org/project/tabulate/)\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load in the penguins\n",
    "penguins = sns.load_dataset(\"penguins\")\n",
    "display(penguins.head())\n",
    "print(penguins.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b23234",
   "metadata": {},
   "source": [
    "![penguins](./images/penguins.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed461ac1",
   "metadata": {},
   "source": [
    "Let's assume we've already done some exploratory data analysis (EDA) to see the distributions of each feature, the potential relationships between them, etc.,  and we're ready to do some modeling. As usual, we'll start by separating our features from our target classes using [Scikit-learn's `test_train_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Since we're trying to predict the species of a penguin, that will be our target, `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa6945e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset    X shape    y shape\n",
      "---------  ---------  ---------\n",
      "Original   (344, 6)   (344,)\n",
      "Training   (275, 6)   (275,)\n",
      "Testing    (69, 6)    (69,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features from target\n",
    "X = penguins.drop('species', axis=1)\n",
    "y = penguins['species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Print out the sizes\n",
    "shape_table = [['Original', X.shape, y.shape], ['Training', X_train.shape, y_train.shape], \n",
    "         ['Testing', X_test.shape, y_test.shape]]\n",
    "print(tabulate(shape_table, headers=['Dataset', 'X shape', 'y shape']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd0fcc9",
   "metadata": {},
   "source": [
    "Before we get too far, we better take a look at a count of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c33bc380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species               0\n",
       "island                0\n",
       "bill_length_mm        2\n",
       "bill_depth_mm         2\n",
       "flipper_length_mm     2\n",
       "body_mass_g           2\n",
       "sex                  11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c86242",
   "metadata": {},
   "source": [
    "Hmm...not too many, but we still have to do something about them before we try to fit a model. Since we don't have that much data to begin with, and because this tutorial depends on it, let's fill them in instead of dropping them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699504d",
   "metadata": {},
   "source": [
    "So the next steps we need to take are:\n",
    "1. Fill missing values with:\n",
    "    - The mean for numerical features\n",
    "    - The mode for categorical features\n",
    "2. Scale the numerical data\n",
    "3. One-hot-encode the categorical data\n",
    "4. Fit a model (we'll just use a simple logistic regression)\n",
    "5. Evaluate the model\n",
    "\n",
    "And we're going to use pipelines to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b753f",
   "metadata": {},
   "source": [
    "![let's do this](./images/lets_do_this.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650f048",
   "metadata": {},
   "source": [
    "## But what's wrong with the way I do things now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f765510",
   "metadata": {},
   "source": [
    "Before pipelines, my workflow might have looked something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998b0652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset      Training score    Cross-val score\n",
      "---------  ----------------  -----------------\n",
      "Original           0.996364           0.992727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# I want to fill in missing values, \n",
    "# but some of my columns are categorical and some are numerical\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Apply each imputer to the correct columns by selecting datatypes\n",
    "X_train_num_imputed = num_imputer.fit_transform(X_train.select_dtypes(include=['int64', 'float64']))\n",
    "X_train_cat_imputed = cat_imputer.fit_transform(X_train.select_dtypes(include='object'))\n",
    "\n",
    "# Might as well scale the numerical stuff...\n",
    "ss = StandardScaler()\n",
    "X_train_num_imputed_scaled = ss.fit_transform(X_train_num_imputed)\n",
    "\n",
    "# ...and one-hot-encode the categorical stuff\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "X_train_cat_imputed_ohe = ohe.fit_transform(X_train_cat_imputed)\n",
    "\n",
    "# Now I gotta put 'em back together\n",
    "X_train_preprocessed = np.concatenate([X_train_num_imputed_scaled, X_train_cat_imputed_ohe], axis=1)\n",
    "\n",
    "# And finally fit and evaluate the model\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train_preprocessed, y_train)\n",
    "initial_score = logreg.score(X_train_preprocessed, y_train)\n",
    "initial_crossval_score = cross_val_score(logreg, X_train_preprocessed, y_train).mean()\n",
    "\n",
    "# Print out scores\n",
    "scores_table = [['Original', initial_score, initial_crossval_score]]\n",
    "scores_headers = ['Dataset', 'Training score', 'Cross-val score']\n",
    "print(tabulate(scores_table, headers=scores_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74de240",
   "metadata": {},
   "source": [
    "***What a mess!***\n",
    "\n",
    "If you skipped over that block of code, I can't blame you. It's repetitive and not well organized. I split and renamed my dataset half a dozen times, then put it all back together. The first time I tried to run it, I had to debug multiple errors, most of which resulted from simple typos because of all the different names I wrote for each new version of `X_train`. If I want to change or add anything later on, I'll have to hunt through my code and rename a bunch of things. And worst of all, I'll have to repeat that *entire* process with `X_test` in order to do a final evaluation of my model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44a435f",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 12;font-style: italic;\"><img style=\"display: block;\" src=\"./images/betterway.gif\" alt=\"There's got to be a better way!\"><center><i>There's got to be a better way!</i></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e29a7",
   "metadata": {},
   "source": [
    "## Pipelines: A Better Way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd962e",
   "metadata": {},
   "source": [
    "Using pipelines will make that code simpler, cleaner, and less repetitive. When working with machine learning models, pipelines make it easier to preprocess data and fit models to training and testing sets. Pipelines can also prevent data leakage, especially when evaluating models via cross-validation. \n",
    "\n",
    "Just as functions store processes you can run again and again, [sklearn's Pipeline class](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) stores instances of other classes to run on your dataset. Some classes are transformers, while others are estimators. Transformers battle the Decepticons--wait, that's not right. \n",
    " - **Transformers** process or alter your data: [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) could both be used in a transformer.\n",
    " - **Estimators** fit a model to your data: [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and [`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) are both sklearn estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b72bbe",
   "metadata": {},
   "source": [
    "Part of what makes pipelines so amazing is their intuitive use of sklearn's consistent API. Anything you can do with a transformer or an estimator on its own you can do with a pipeline. That means you can use methods like `.fit()`, `.transform()`, or `.predict()` on a pipeline just like you can on each individual piece. You can also use a pipeline in cross-validation to evaluate the performance of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732ae24",
   "metadata": {},
   "source": [
    "## How do you build a pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd2155",
   "metadata": {},
   "source": [
    "When you instantiate sklearn's Pipeline class, the main parameter you need to define is `steps`, which takes a list of the transformers and estimator you'd like to include in the pipeline. Each step is written in the form of a [tuple](https://www.w3schools.com/python/python_tuples.asp). The first item in the tuple is a `string` name for the transformer/estimator and the second item is the transformer/estimator itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deebdb2",
   "metadata": {},
   "source": [
    "A typical pipeline might contain multiple transformers and a final estimator, but you don't necessarily need multiple components in every pipeline. In fact, the simplest possible pipeline contains just one thing:\n",
    "\n",
    "![simple pipeline](./images/simple_pipe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fcb8d2",
   "metadata": {},
   "source": [
    "In practice, pipelines usually have at least two steps. Ultimately, we're going to build a pipeline that consists of other pipelines! \n",
    "\n",
    "To build a pipeline, you need to know the steps you want to take to process your data and fit a model. That can be difficult to know entirely in advance, so it's sometimes easier to start with some messy code and then refactor it. Another strategy is build smaller pipelines before putting them together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1cedc",
   "metadata": {},
   "source": [
    "### Pipe-by-numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304c354",
   "metadata": {},
   "source": [
    "Let's start with a pipeline for just the numerical columns in the dataset. We'll impute the null values using the mean for each column, then scale the data before fitting with a `LogisticRegression()` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3382651",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset      Training score    Cross-val score\n",
      "---------  ----------------  -----------------\n",
      "Original           0.996364           0.992727\n",
      "Numerical          0.989091           0.985455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipeline for numerical data only\n",
    "num_pipe = Pipeline(steps=[\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('ss', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Select only the numerical columns and drop all nulls\n",
    "X_train_numerical = X_train.select_dtypes(include='float64')\n",
    "\n",
    "# Fit and score the pipeline\n",
    "num_pipe.fit(X_train_numerical, y_train)\n",
    "num_score = num_pipe.score(X_train_numerical, y_train)\n",
    "num_crossval_score = cross_val_score(num_pipe, X_train_numerical, y_train).mean()\n",
    "\n",
    "# Compare scores\n",
    "scores_table.append(['Numerical', num_score, num_crossval_score]) \n",
    "print(tabulate(scores_table, headers=scores_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff61b2a",
   "metadata": {},
   "source": [
    "It makes sense that our scores dipped--remember that the Original score included all the features from the original dataset, while Numerical includes only the numerical features.\n",
    "\n",
    "Compare the syntax to accomplish processing the data and fitting a model with and without a pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e2004",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "```python\n",
    "# With a pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('num_imputer', SimpleImputer()),\n",
    "    ('ss', StandardScaler()),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b5a8f7",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "```python\n",
    "# Without a pipeline\n",
    "imputer = SimpleImputer()\n",
    "ss = StandardScaler()\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_train_scl = ss.fit_transform(X_train_imp)\n",
    "logreg.fit(X_train_scl)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c5761",
   "metadata": {},
   "source": [
    "Look how using a pipeline reduced the overall amount of code and completely removed the need to create a new, renamed version of `X_train` for each step. Simpler, less risk of errors, and very DRY!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc561dd",
   "metadata": {},
   "source": [
    "### A Categorical Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3ce71",
   "metadata": {},
   "source": [
    "Can we add back in our categorical columns? You betcha! For this we'll pull in another class called [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html). ColumnTransformer is incredibly useful in exactly these kinds of situations because it allows us to perform different operations on different columns, all in one go.\n",
    "\n",
    "To use ColumnTransformer, we'll refactor our code a bit and create two sub-pipelines: one for  numerical data and one for  categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb679896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-pipeline for the numerical columns\n",
    "num_transformer = Pipeline(steps=[\n",
    "                           ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "                           ('ss', StandardScaler())])\n",
    "\n",
    "# Sub-pipeline for the categorical columns\n",
    "cat_transformer = Pipeline(steps=[\n",
    "                           ('cat_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                           ('ohe', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2cc44",
   "metadata": {},
   "source": [
    "Notice how neither of these pipelines ends with our LogisticRegression estimator! We'll save that for our final pipeline. Instead, we're going to join these two sub-pipelines together using a ColumnTransformer, which takes a list of the transformers you'd like to include in the pipeline. Each transformer is written in the form of a 3-tuple with the following items: \n",
    "1. The name for the transformer (a `string`)\n",
    "2. The class or instance of a transformer or sub-pipeline\n",
    "3. The columns to apply the transformer to\n",
    "\n",
    "We can specify the columns by giving a list, such as `['bill_length_mm', 'bill_depth_mm']`, but using [`make_columns_selector`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html) is easier since we're selecting columns by datatype rather than by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d4e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical sub-pipe', num_transformer, make_column_selector(dtype_include=['float64'])),\n",
    "        ('categorical sub-pipe', cat_transformer, make_column_selector(dtype_include=['object']))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91855c",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e0ba7",
   "metadata": {},
   "source": [
    "Now we can make a complete pipeline that preprocesses all our features and ends with our estimator. Notice now in the `'preprocessing'` step we're passing in the ColumnTransformer that contains the two sub-pipelines, then letting the LogisticRegression work its magic on our whole, completely processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f16dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A complete pipeline \n",
    "complete_pipe = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('logreg', LogisticRegression(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28df2e4d",
   "metadata": {},
   "source": [
    "The complete pipeline now consists of a ColumnTransformer and a LogisticRegression classifier. Inside the ColumnTransformer are two sub-pipelines, one for each datatype in our dataset. Each sub-pipeline is made of a SimpleImputer and one other step: a StandardScaler for the numerical data and a OneHotEncoder for the categorical data.\n",
    "\n",
    "No worries if that's still a lot of pieces to put together. It might be easier to just see a visual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd8b6c2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}</style><div class=\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"6370c2a0-0b26-46d5-9039-aabb0e612672\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"6370c2a0-0b26-46d5-9039-aabb0e612672\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('numerical sub-pipe',\n",
       "                                                  Pipeline(steps=[('num_imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('ss',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7fcb432deb20>),\n",
       "                                                 ('categorical sub-pipe',\n",
       "                                                  Pipeline(steps=[('cat_imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7fcb434b0310>)])),\n",
       "                ('logreg', LogisticRegression(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"9be1c241-3de4-4075-b445-e79fd04df8f4\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"9be1c241-3de4-4075-b445-e79fd04df8f4\">preprocessing: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('numerical sub-pipe',\n",
       "                                 Pipeline(steps=[('num_imputer',\n",
       "                                                  SimpleImputer()),\n",
       "                                                 ('ss', StandardScaler())]),\n",
       "                                 <sklearn.compose._column_transformer.make_column_selector object at 0x7fcb432deb20>),\n",
       "                                ('categorical sub-pipe',\n",
       "                                 Pipeline(steps=[('cat_imputer',\n",
       "                                                  SimpleImputer(strategy='most_frequent')),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                 <sklearn.compose._column_transformer.make_column_selector object at 0x7fcb434b0310>)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b2bdf064-d06d-4036-9f47-43b2e55c6daf\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"b2bdf064-d06d-4036-9f47-43b2e55c6daf\">numerical sub-pipe</label><div class=\"sk-toggleable__content\"><pre><sklearn.compose._column_transformer.make_column_selector object at 0x7fcb432deb20></pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"55d58a82-ebc4-4185-8d87-784187d73c6e\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"55d58a82-ebc4-4185-8d87-784187d73c6e\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c834482f-f26e-48dc-b33d-d5eef1b06f47\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c834482f-f26e-48dc-b33d-d5eef1b06f47\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"dbdf33bc-76ec-411f-adf8-2603868717f4\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"dbdf33bc-76ec-411f-adf8-2603868717f4\">categorical sub-pipe</label><div class=\"sk-toggleable__content\"><pre><sklearn.compose._column_transformer.make_column_selector object at 0x7fcb434b0310></pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"669dbe7e-f3e4-4b22-a448-8a428e2727a6\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"669dbe7e-f3e4-4b22-a448-8a428e2727a6\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy='most_frequent')</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"30f6e5d7-f3ac-4921-9e86-23c2608f5480\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"30f6e5d7-f3ac-4921-9e86-23c2608f5480\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown='ignore')</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"0eb8c202-99a8-4003-8b24-c343031fb590\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"0eb8c202-99a8-4003-8b24-c343031fb590\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('numerical sub-pipe',\n",
       "                                                  Pipeline(steps=[('num_imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('ss',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7fcb432deb20>),\n",
       "                                                 ('categorical sub-pipe',\n",
       "                                                  Pipeline(steps=[('cat_imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7fcb434b0310>)])),\n",
       "                ('logreg', LogisticRegression(random_state=42))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will allow us to see a nice diagram of our pipeline\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "complete_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef7e48",
   "metadata": {},
   "source": [
    "*Note: If you're running this code yourself in a Jupyter Notebook, see what happens when you click on each component in the diagram!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83c73a",
   "metadata": {},
   "source": [
    "Now when we want to fit and score our pipeline, we don't have to select columns or datatypes outside of it--it all happens within the pipe! We can now pass in `X_train` directly, without having to transform it by hand or meddle with it at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e71f0458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset      Training score    Cross-val score\n",
      "---------  ----------------  -----------------\n",
      "Original           0.996364           0.992727\n",
      "Numerical          0.989091           0.985455\n",
      "Complete           0.996364           0.996364\n"
     ]
    }
   ],
   "source": [
    "# Fit and score the pipeline\n",
    "complete_pipe.fit(X_train, y_train)\n",
    "complete_score = complete_pipe.score(X_train, y_train)\n",
    "complete_crossval_score = cross_val_score(complete_pipe, X_train, y_train).mean()\n",
    "\n",
    "# Compare scores\n",
    "scores_table.append(['Complete', complete_score, complete_crossval_score]) \n",
    "print(tabulate(scores_table, headers=scores_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ffb6f",
   "metadata": {},
   "source": [
    "As expected, our Complete training score is identical to our Original since we're once again using all our features again. But notice that the cross-val scores are different! (Hint: It has to do with accidental data leakage during the construction of the Original model! But we'll save that for another post...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397a3ac",
   "metadata": {},
   "source": [
    "![you did it!](./images/you_did_it.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf6d6d",
   "metadata": {},
   "source": [
    "## Not to repeat myself..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f711ee",
   "metadata": {},
   "source": [
    "...but let's take another look at both methods to see the full pipeline all together and to admire the power of the pipe to make our code simpler, cleaner, and as DRY an article about pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c781c2",
   "metadata": {},
   "source": [
    "### Without pipelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afdd2fd",
   "metadata": {},
   "source": [
    "*If you skipped over this code block before, this time try to identify each part that we included in our complete pipeline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "675b0ead",
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset      Training score    Cross-val score\n",
      "---------  ----------------  -----------------\n",
      "Original           0.996364           0.992727\n"
     ]
    }
   ],
   "source": [
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "X_train_num_imputed = num_imputer.fit_transform(X_train.select_dtypes(include=['int64', 'float64']))\n",
    "X_train_cat_imputed = cat_imputer.fit_transform(X_train.select_dtypes(include='object'))\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_num_imputed_scaled = ss.fit_transform(X_train_num_imputed)\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "X_train_cat_imputed_ohe = ohe.fit_transform(X_train_cat_imputed)\n",
    "\n",
    "X_train_preprocessed = np.concatenate([X_train_num_imputed_scaled, X_train_cat_imputed_ohe], axis=1)\n",
    "\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train_preprocessed, y_train)\n",
    "initial_score = logreg.score(X_train_preprocessed, y_train)\n",
    "initial_crossval_score = cross_val_score(logreg, X_train_preprocessed, y_train).mean()\n",
    "\n",
    "scores_table = [['Original', initial_score, initial_crossval_score]]\n",
    "scores_headers = ['Dataset', 'Training score', 'Cross-val score']\n",
    "print(tabulate(scores_table, headers=scores_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e195a",
   "metadata": {},
   "source": [
    "Without pipelines, we have to:\n",
    " - perform each step manually\n",
    " - keep track of various versions and splits of `X_train`\n",
    " - join them all back together in the end. \n",
    " \n",
    "It's complex, repetitive, and at high risk of errors from typos or putting steps in the wrong order. Not to mention that there's data leakage (hint: it has to do with StandardScaler!). And worst of all, in order to evaluate our model on our holdout set, we'd have to repeat the *entire process* with a whole new set of versions and splits of `X_test`, plus remembering to change each `.fit_transform()` to `.transform()` and to remove `logreg.fit()` entirely. It's a recipe for endless debugging and invalid results. No thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27507474",
   "metadata": {},
   "source": [
    "### With pipelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4f613",
   "metadata": {},
   "source": [
    "*Here's our pipeline, all in one go.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a76ba4d9",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset      Training score    Cross-val score\n",
      "---------  ----------------  -----------------\n",
      "Original           0.996364           0.992727\n",
      "Complete           0.996364           0.996364\n"
     ]
    }
   ],
   "source": [
    "num_transformer = Pipeline(steps=[\n",
    "                           ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "                           ('ss', StandardScaler())])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "                           ('cat_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                           ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical sub-pipe', num_transformer, make_column_selector(dtype_include=['float64'])),\n",
    "        ('categorical sub-pipe', cat_transformer, make_column_selector(dtype_include=['object']))\n",
    "    ])\n",
    "\n",
    "complete_pipe = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('logreg', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "complete_pipe.fit(X_train, y_train)\n",
    "complete_score = complete_pipe.score(X_train, y_train)\n",
    "complete_crossval_score = cross_val_score(complete_pipe, X_train, y_train).mean()\n",
    "\n",
    "scores_table.append(['Complete', complete_score, complete_crossval_score]) \n",
    "print(tabulate(scores_table, headers=scores_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc1318",
   "metadata": {},
   "source": [
    "With pipelines, our code is cleaner and each step is clearly spelled out. Making changes or adding to any of the parts of the complete pipeline is easy and doesn't require a cascade of renaming. Plus, in order to evaluate our model on our holdout set, all we have to do is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e1d685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score on holdout set:  0.9855072463768116\n"
     ]
    }
   ],
   "source": [
    "final_score = complete_pipe.score(X_test, y_test)\n",
    "print('Final score on holdout set: ', final_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c9936",
   "metadata": {},
   "source": [
    "![it's that simple](./images/simple.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847b144",
   "metadata": {},
   "source": [
    "### But wait, there's more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b1c30",
   "metadata": {},
   "source": [
    "If you want to level-up your pipelines, check out these other methods:\n",
    "\n",
    " - Add your own custom functions with [FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html)!\n",
    " - [FeatureUnion](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) for combining transformers in parallel!\n",
    " - Performing a [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) on a pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e6f53",
   "metadata": {},
   "source": [
    "I hope this helped you transition into using pipelines! Learning about pipelines brought a lot of clarity to my understanding of machine learning and a lot of improvements to my code. I'm still fairly new to Data Science, so please feel free to leave a suggestion or (especially) a correction in the comments!\n",
    "\n",
    "Happy modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20788227",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
